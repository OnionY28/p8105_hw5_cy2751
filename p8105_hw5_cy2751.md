p8105_hw5_cy2752
================
Congyu Yang
2024-11-09

## Problem 1

## Problem 2

``` r
randomset <-  function(n = 30,mu,sigma = 5) {
  
  x = rnorm(n,mu,sigma)
  
  x %>%t.test()%>% 
    broom::tidy() %>% select(estimate,p.value) %>% 
    mutate(power = ifelse(p.value < 0.05,T,F),
           mu = mu)
}

output = vector("list", 50)
output_set <- vector("list",6)

for (j in 1:6) {
  for (i in 1:50) {
  
  output[[i]] = randomset(mu = j)
  
  }
  output_set[[j]] <- bind_rows(output)
}
```

## Make a plot

``` r
output_set %>%
  bind_rows() %>% 
  group_by(mu) %>% 
  summarise(prop_power = mean(power)) %>% 
  ggplot(aes(x = mu, y =prop_power)) + 
  geom_point()+geom_line()
```

<img src="p8105_hw5_cy2751_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

As we can see, as the true value of $\mu$ going larger, the power is
approaching towards 1, since we are always doing the test comparing if
the mean is equal to 0, so as the true mean moves away from 0, the power
of the test is getting higher.

``` r
mu_compare_all <- output_set %>%
  bind_rows() %>% 
  group_by(mu) %>% 
  summarise(avg_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y =avg_estimate)) + 
  geom_point()+geom_line()+
  xlim(0,7)+ ylim(0,7)

mu_compare_only_reject <-output_set %>%
  bind_rows() %>% 
  filter(power == T) %>% 
  group_by(mu) %>% 
  summarise(avg_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y =avg_estimate)) + 
  geom_point()+geom_line()+
  xlim(0,7)+ ylim(0,7)

mu_compare_all + mu_compare_only_reject
```

<img src="p8105_hw5_cy2751_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

The sample average of $\hat{\mu}$ across tests for which the null is
rejected is not equal to(larger than) the true value of $\mu$ when the
$\mu$ is smaller, but as $\mu$ getting far away from 0, they are getting
closer and equal to each other.  

This is because our test is about whether $\mu$ = 0, so when we do this
test on small $\mu$, samples $\hat{\mu}$ across tests for which the null
is rejected are far away from 0, so it will also be away from the true
value of $\mu$.  

While for larger $\mu$, almost all samples have their null hypothesis
rejected, so it is actually compare the average of all estimate of
$\hat{\mu}$ and the true value of $\mu$, accordingly, sample average of
$\hat{\mu}$ across tests for which the null is rejected is equal to the
true value of $\mu$ when $\mu$ gets far away from 0.

# Problem 3

``` r
homicide <- read_csv("data/homicide-data.csv") %>% 
  mutate(city_state = str_c(city,state,sep = ", "))
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
total_homicide <- homicide %>% 
  group_by(city_state) %>% 
  summarize(total_cases = n()) 

unsolved_homiside <- homicide %>% 
  filter(disposition == "Closed without arrest" | disposition == "Open/No arrest") %>% 
  group_by(city_state) %>% 
  summarize(unsolved_cases = n()) 

(cases_table <- left_join(total_homicide,unsolved_homiside, by = "city_state") %>% 
    mutate(
    unsolved_cases = case_when(is.na(unsolved_cases) ~ 0,TRUE ~ unsolved_cases)
    ))
```

    ## # A tibble: 51 × 3
    ##    city_state      total_cases unsolved_cases
    ##    <chr>                 <int>          <dbl>
    ##  1 Albuquerque, NM         378            146
    ##  2 Atlanta, GA             973            373
    ##  3 Baltimore, MD          2827           1825
    ##  4 Baton Rouge, LA         424            196
    ##  5 Birmingham, AL          800            347
    ##  6 Boston, MA              614            310
    ##  7 Buffalo, NY             521            319
    ##  8 Charlotte, NC           687            206
    ##  9 Chicago, IL            5535           4073
    ## 10 Cincinnati, OH          694            309
    ## # ℹ 41 more rows

This dataset about homicides in 50 large U.S. cities has 52179 rows and
13 columns. It has more than 52000 criminal homicides over the past
decade. And it includes the date of the cases，basic information about
the suspect，location of the cases and whether the arrest has been made.

``` r
BalMD <- cases_table %>% 
  filter(city_state == "Baltimore, MD")

prop_test_BalMD <-
  prop.test(x = BalMD %>% pull(unsolved_cases), n = BalMD %>% pull(total_cases)) %>%
  broom::tidy() %>% 
  select(estimate,conf.low,conf.high) %>% 
  mutate(city = BalMD %>% select(city_state)) %>% 
  unnest()

est_and_CI <- function(state){
  
  City <- cases_table %>% filter(city_state == state)
  
  prop.test(x = City %>% pull(unsolved_cases), n = City %>% pull(total_cases)) %>%
  broom::tidy() %>% 
  select(estimate,conf.low,conf.high) %>% 
    mutate(city = City %>% select(city_state)) %>% 
  unnest()
  
}


prop_test_all <- map_dfr(cases_table %>% pull(city_state), \(x) est_and_CI(x))
```

``` r
prop_test_all %>% 
  arrange(desc(estimate)) %>% 
  mutate(city_state = reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
    geom_point()+
  theme(axis.text.x = element_text(size = 5,angle = 30))
```

<img src="p8105_hw5_cy2751_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />
